{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_A7.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG2fLD44KJhlL5Cu5zvh62",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjankraj/Shala/blob/main/ML_A7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ep_wCwyTaLED"
      },
      "source": [
        "import numpy as np\r\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ghsDPGbKfy"
      },
      "source": [
        "class Activation(object):\r\n",
        "\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.state = None\r\n",
        "\r\n",
        "\r\n",
        "  def __call__(self,x):\r\n",
        "    return self.forward(x)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    raise NotImplemented\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    raise NotImplemented"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTHT4H7beAB7"
      },
      "source": [
        "class Identity(Activation):\r\n",
        "\r\n",
        "  def __init(self):\r\n",
        "    super(Identity, self).__init__()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    self.state = x\r\n",
        "    return x\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    return 1.0"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ1ScQkje8uA"
      },
      "source": [
        "class Simoid(Activation):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super(Sigmoid, self).__init__()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    self.state = 1.0 / (1.0 + np.exp(-x) )\r\n",
        "    return self\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    return self.state * (1 - self.state)  "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwxm77iIgO5p"
      },
      "source": [
        "class Tanh(Activation):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super(Tanh, self).__init__()\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "    e_minus_x = np.exp(-x)\r\n",
        "    e_plus_x = np.exp(x)\r\n",
        "\r\n",
        "    num = e_plus_x - e_minus_x\r\n",
        "    den = e_plus_x + e_minus_x\r\n",
        "\r\n",
        "    self.state = num / den\r\n",
        "    return self.state\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    return 1 - (self.state * self.state)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkKgd3u-hto4"
      },
      "source": [
        "class ReLU(Activation):\r\n",
        "\r\n",
        "  def __init__(self, x):\r\n",
        "    super (ReLU, x).__init__()\r\n",
        "\r\n",
        "  def forward(self):\r\n",
        "    self.state = np.where(x > 0, x, 0.0)\r\n",
        "    return self.state\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    relu_d = (lambda x: 1 if self.state > 0 else 0)\r\n",
        "    return relu_d"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZW07RHyjSU6"
      },
      "source": [
        "class Criterion(object):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    self.logits = None\r\n",
        "    self.labels = None\r\n",
        "    self.loss = None\r\n",
        "\r\n",
        "  def __call__(self, x, y):\r\n",
        "    return seld.forward(x,y)\r\n",
        "\r\n",
        "  def forward(self, x, y):\r\n",
        "    raise NotImplemented\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    raise NotImplemented  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Tj_tt9kzMW"
      },
      "source": [
        "class SoftmaxCrossEntropy(Criterion):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super (SoftmaxEntropy, self).__init__()\r\n",
        "\r\n",
        "  def forward(self, x, y):\r\n",
        "\r\n",
        "    self.logits = x\r\n",
        "    self.labels = y\r\n",
        "\r\n",
        "    #Numerically stable softmax\r\n",
        "    mx = np.max(self.logits, axis=1).reshape(-1,1)\r\n",
        "    subtracted = self.logits - mx\r\n",
        "    self.exp_logits = np.exp(subtracted)\r\n",
        "    exp_sum = self.exp_logits.sum(axis=1).reshape(-1,1)\r\n",
        "    self.sm = self.exp_logits / exp_sum\r\n",
        "\r\n",
        "    # Cross entropy \r\n",
        "    first_term = - (self.logits * self.labels).sum(axis=1)\r\n",
        "    second_term = mx + np.log(exp_sum)\r\n",
        "    \r\n",
        "    return first_term + second_term.reshape(-1)\r\n",
        "\r\n",
        "\r\n",
        "  def derivative(self):\r\n",
        "    return self.sm - self.labels"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzMl-IwfnQbi"
      },
      "source": [
        "class Linear():\r\n",
        "\r\n",
        "  def __init__(self, in_feature, out_feature, weight_init_fn, bias_init_fn):\r\n",
        "    \r\n",
        "    self.W = weight_init_fn(in_feature, out_feature)\r\n",
        "    self.b = bias_init_fn(out_feature)\r\n",
        "\r\n",
        "    self.dW = np.zeros(self.W.shape)\r\n",
        "    self.db = np.zeros(self.b.shape)\r\n",
        "\r\n",
        "    self.momentum_W = np.zeros(self.W.shape)\r\n",
        "    self.momentum_b = np.zeros(self.b.shape)\r\n",
        "\r\n",
        "  \r\n",
        "  def __call__(self, x):\r\n",
        "    return self.forward(x)\r\n",
        "\r\n",
        "  def forward(self, x):\r\n",
        "\r\n",
        "    self.x = x\r\n",
        "    out = np.matmul(self.x, self.W) + self.b\r\n",
        "    return out\r\n",
        "\r\n",
        "\r\n",
        "  def backward(self, delta):\r\n",
        "    self.dW = np.dot(self.x.T, delta) / delta.shape[0]\r\n",
        "    self.db = np.sum(delta, axis=0, keepdims=True) / delta.shape[0]\r\n",
        "\r\n",
        "    dx = np.dot(delta, self.W.T)\r\n",
        "    return dx"
      ],
      "execution_count": 10,
      "outputs": []
    }
  ]
}