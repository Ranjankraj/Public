{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multiclass_20_ng.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNxYzdCeMDZKONuUPFC8jdY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjankraj/Shala/blob/main/multiclass_20_ng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXiXplqjdnUv",
        "outputId": "e16b11bc-0ac4-4cd8-b8cb-e76a3d2505e4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSj86iV2gaNR"
      },
      "source": [
        "# !unzip  \"/content/20_newsgroups.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrTgmanbdXmU"
      },
      "source": [
        "!unzip -u \"/content/20_newsgroups.zip\" -d \"/content/Internship/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxyXtbbwi8fU",
        "outputId": "e692b459-2d9e-4c63-f55b-679e26735183"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20_newsgroups.zip  drive  Internship  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vgg97b63eGnd"
      },
      "source": [
        "from os import listdir\r\n",
        "from os.path import isfile, join\r\n",
        "import string"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM4sI9VEeOWM"
      },
      "source": [
        "my_path = \"/content/Internship/20_newsgroups\"\r\n",
        "\r\n",
        "#creating a list of folder names to make valid pathnames later\r\n",
        "folders = [f for f in listdir(my_path)]"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBlAvlAYeuQM",
        "outputId": "e87dce16-2df3-4e87-cf12-73a25e1bfe9b"
      },
      "source": [
        "folders"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rec.sport.hockey',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'alt.atheism',\n",
              " 'sci.crypt',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'sci.med',\n",
              " 'sci.electronics',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'misc.forsale',\n",
              " 'comp.windows.x',\n",
              " 'talk.religion.misc',\n",
              " 'rec.autos',\n",
              " 'talk.politics.misc',\n",
              " 'rec.sport.baseball',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.mideast',\n",
              " 'rec.motorcycles',\n",
              " 'comp.graphics',\n",
              " 'sci.space',\n",
              " 'talk.politics.guns']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5CyFjoVfKSc",
        "outputId": "17c1e55a-1ae7-4997-979d-da8ed7484173"
      },
      "source": [
        "print(len(folders))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqC06Irbs1Pn"
      },
      "source": [
        "#function to remove incompatible files\r\n",
        "import os\r\n",
        "import sklearn.datasets\r\n",
        "def find_incompatible_files(path):\r\n",
        "    \"\"\"\r\n",
        "    Finds the filenames that are incompatible with `CountVectorizer`. These files are usually not compatible with UTF8!\r\n",
        "    parameter `path` is the absolute or relative path of the training data's root directory.\r\n",
        "    returns a list of strings.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    count_vector = sklearn.feature_extraction.text.CountVectorizer()\r\n",
        "    files = sklearn.datasets.load_files(path)\r\n",
        "    num = []\r\n",
        "    for i in range(len(files.filenames)):\r\n",
        "        try:\r\n",
        "            count_vector.fit_transform(files.data[i:i + 1])\r\n",
        "        except UnicodeDecodeError:\r\n",
        "            num.append(files.filenames[i])\r\n",
        "            os.remove(files.filenames[i])\r\n",
        "        except ValueError:\r\n",
        "            pass\r\n",
        "\r\n",
        "    return num\r\n"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4wTSL1EtzxP"
      },
      "source": [
        "#Removing incompatible files\r\n",
        "val = find_incompatible_files(my_path)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVY8oXoUHuBi",
        "outputId": "0e57ed75-164c-4c47-fc73-c07501ce1a99"
      },
      "source": [
        "#printing no. of incompatible files removed\r\n",
        "\r\n",
        "print(len(val))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "73\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKf-cweofQZc"
      },
      "source": [
        "#creating a 2D list to store list of all files in different folders\r\n",
        "\r\n",
        "files = []\r\n",
        "for folder_name in folders:\r\n",
        "    folder_path = join(my_path, folder_name)\r\n",
        "    files.append([f for f in listdir(folder_path)])"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHz0QeWZfylM",
        "outputId": "247e1f58-cc82-41b2-9690-6f89dd77b05c"
      },
      "source": [
        "sum(len(files[i]) for i in range(20))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVbdp-atjSKV"
      },
      "source": [
        "pathname_list = []\r\n",
        "for fo in range(len(folders)):\r\n",
        "    for fi in files[fo]:\r\n",
        "        pathname_list.append(join(my_path, join(folders[fo], fi)))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JT9rAI-jZXi",
        "outputId": "d466cb01-30a7-4b4d-e689-0f042883bdd1"
      },
      "source": [
        "len(pathname_list)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4-kWBNOjbq0"
      },
      "source": [
        "#making an array containing the classes each of the documents belong to\r\n",
        "\r\n",
        "Y = []\r\n",
        "for folder_name in folders:\r\n",
        "    folder_path = join(my_path, folder_name)\r\n",
        "    num_of_files= len(listdir(folder_path))\r\n",
        "    for i in range(num_of_files):\r\n",
        "        Y.append(folder_name)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0OtSmZaji2Z",
        "outputId": "2d1ab50b-5cee-4ac2-bed0-3c4c3c2e1349"
      },
      "source": [
        "len(Y)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19924"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cBaW3e7AFsy"
      },
      "source": [
        "#function for cleaning and reading data\r\n",
        "\r\n",
        "import collections\r\n",
        "import os.path\r\n",
        "import glob\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "def cleanDoc(doc):\r\n",
        "    tokens = nltk.WordPunctTokenizer().tokenize(doc)\r\n",
        "    # stop_words = set(stopwords.words('english'))\r\n",
        "    # tokens = remove_stopwords(tokens)\r\n",
        "    clean = [token.lower() for token in tokens if len(token) > 3 and token.isalpha()]\r\n",
        "    # clean = [word for word in clean if not word in stopwords]\r\n",
        "    return clean\r\n",
        "\r\n",
        "\r\n",
        "def ps(doc):\r\n",
        "      f = open(doc)\r\n",
        "      data= f.read()\r\n",
        "      words = cleanDoc(data)\r\n",
        "      return words\r\n",
        "\r\n",
        "\r\n",
        "#Storing list of all the words\r\n",
        "\r\n",
        "list_of_all_words = []\r\n",
        "\r\n",
        "for i in range(len(pathname_list)):\r\n",
        "  for file in glob.glob(pathname_list[i]):\r\n",
        "      \r\n",
        "      words = ps(file)\r\n",
        "      for word in words:\r\n",
        "        list_of_all_words.append(word)\r\n",
        "\r\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeIRYEBKA7ME",
        "outputId": "ef6cc172-355a-456a-a07e-fa7435f1a8b2"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "np_list_of_all_words = np.asarray(list_of_all_words)\r\n",
        "\r\n",
        "words, counts = np.unique(np_list_of_all_words, return_counts=True)\r\n",
        "len(words)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "95960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_smrOnBpC2N1"
      },
      "source": [
        "def count(word):\r\n",
        "  d_fq = []\r\n",
        "  for i in range(len(pathname_list)):\r\n",
        "    for file in glob.glob(pathname_list[i]):\r\n",
        "\r\n",
        "        f = open(file)\r\n",
        "        data= f.read()\r\n",
        "        words = cleanDoc(data)\r\n",
        "        if word in words:\r\n",
        "          d_fq.append(pathname_list[i])\r\n",
        "  return len(d_fq)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8gm5U0FTzx_"
      },
      "source": [
        "#dictionary will contain unique words mapped with their document frequency\r\n",
        "d_words = {}\r\n",
        "\r\n",
        "for word in words: d_words[word] = count(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSra35jNDb5-"
      },
      "source": [
        "print(d_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CgPxa2GEAPd"
      },
      "source": [
        "#Sorting dictionary\r\n",
        "import operator\r\n",
        "\r\n",
        "sorted_d = dict( sorted(d_words.items(), key=operator.itemgetter(1),reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNOz-rSVFAE-"
      },
      "source": [
        "#choosing 500 to 1000 words by document frequency\r\n",
        "dict_items = sorted_d.keys()\r\n",
        "\r\n",
        "f_t = list(dict_items)[500:1001]\r\n",
        "\r\n",
        "print(f_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv5VoFZwFzZ3"
      },
      "source": [
        "#We can loop over the below function to find Probability(word|class)\r\n",
        "\r\n",
        "def prob(word,class):\r\n",
        "  d_fq = []\r\n",
        "  \r\n",
        "  for folder_name in folders:\r\n",
        "    if class == folder_name:\r\n",
        "      files = []\r\n",
        "      folder_path = join(my_path, folder_name)\r\n",
        "      files.append([f for f in listdir(folder_path)])\r\n",
        "      pathname_list = []\r\n",
        "      for fi in files[fo]:\r\n",
        "          pathname_list.append(join(my_path, join(folders[fo], fi)))\r\n",
        "      for i in range(len(pathname_list)):\r\n",
        "        for file in glob.glob(pathname_list[i]):\r\n",
        "\r\n",
        "          f = open(file)\r\n",
        "          data= f.read()\r\n",
        "          words = cleanDoc(data)\r\n",
        "          if word in words:\r\n",
        "            d_fq.append(pathname_list[i])\r\n",
        "\r\n",
        "          return ((len(d_fq))/(len(pathname_list)))\r\n",
        "\r\n",
        "\r\n",
        "w = 'article'\r\n",
        "c = 'alt.atheism'\r\n",
        "\r\n",
        "prob(w,c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR2rXG7lWnG4"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-1iyIWDjm_w"
      },
      "source": [
        "do_train, do_test, Y_train, Y_test = train_test_split(pathname_list, Y, random_state=0, test_size=0.25)"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVqDo85-nmwU"
      },
      "source": [
        "def flatten(list):\r\n",
        "    new_list = []\r\n",
        "    for i in list:\r\n",
        "        for j in i:\r\n",
        "            new_list.append(j)\r\n",
        "    return new_list"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXeGZlQ_pDH6"
      },
      "source": [
        "list_of_words = []\r\n",
        "\r\n",
        "for document in do_train:\r\n",
        "        list_of_words.append((ps(document)))"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmb26wzyqsDD",
        "outputId": "a1bc1ae9-a7da-4694-904d-57fa7d978dfd"
      },
      "source": [
        "len(list_of_words)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IvWldpzpqKE"
      },
      "source": [
        "import numpy as np\r\n",
        "np_list_of_words = np.asarray(flatten(list_of_words))\r\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvww_Cu_p1Yp",
        "outputId": "391d13b2-29fb-45ba-9bbd-b16213de7e7c"
      },
      "source": [
        "#finding the number of unique words that we have extracted from the documents\r\n",
        "\r\n",
        "words, counts = np.unique(np_list_of_words, return_counts=True)\r\n",
        "len(words)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84071"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPfAhSvusWLr"
      },
      "source": [
        "#sorting the unique words according to their frequency in all the documents\r\n",
        "\r\n",
        "freq, wrds = (list(i) for i in zip(*(sorted(zip(counts, words), reverse=True))))"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eVbLVh5tESc"
      },
      "source": [
        "#deciding the no. of words to use as feature. took n = 1000 to 5000\r\n",
        "\r\n",
        "n = 5001\r\n",
        "features = wrds[1000:n]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ewdGSjntWcT"
      },
      "source": [
        "#creating a dictionary for train that contains each document's vocabulary and ocurence of each word of the vocabulary \r\n",
        "\r\n",
        "dictionary = {}\r\n",
        "doc_num = 1\r\n",
        "for doc_words in list_of_words:\r\n",
        "    #print(doc_words)\r\n",
        "    np_doc_words = np.asarray(doc_words)\r\n",
        "    w, c = np.unique(np_doc_words, return_counts=True)\r\n",
        "    dictionary[doc_num] = {}\r\n",
        "    for i in range(len(w)):\r\n",
        "        dictionary[doc_num][w[i]] = c[i]\r\n",
        "    doc_num = doc_num + 1"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYLE2BIXt0By"
      },
      "source": [
        "#now we make a 2D array having the frequency of each word of our feature set in each individual documents\r\n",
        "\r\n",
        "X_train = []\r\n",
        "for k in dictionary.keys():\r\n",
        "    row = []\r\n",
        "    for f in features:\r\n",
        "        if(f in dictionary[k].keys()):\r\n",
        "            #if word f is present in the dictionary of the document as a key, its value is copied\r\n",
        "            #this gives us no. of occurences\r\n",
        "            row.append(dictionary[k][f]) \r\n",
        "        else:\r\n",
        "            #if not present, the no. of occurences is zero\r\n",
        "            row.append(0)\r\n",
        "    X_train.append(row)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi-jCM0IuDHy"
      },
      "source": [
        "#we convert the X and Y into np array for concatenation and conversion into dataframe\r\n",
        "\r\n",
        "X_train = np.asarray(X_train)\r\n",
        "Y_train = np.asarray(Y_train)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQwWA5i6uMDj",
        "outputId": "e6480eee-a6a1-41a2-d9df-9eaeb67297f0"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbDDj2rZuOGU",
        "outputId": "8c920fcb-a0fa-46ab-c0a1-c27321b5f1fd"
      },
      "source": [
        "len(Y_train)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRULjUnauRw0"
      },
      "source": [
        "list_of_words_test = []\r\n",
        "\r\n",
        "for document in do_test:\r\n",
        "        list_of_words_test.append(ps(document))"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHaKDt6LuZtK"
      },
      "source": [
        "#creating a dictionary for test that contains each document's vocabulary and ocurence of each word of the vocabulary \r\n",
        "\r\n",
        "dictionary_test = {}\r\n",
        "doc_num = 1\r\n",
        "for doc_words in list_of_words_test:\r\n",
        "    #print(doc_words)\r\n",
        "    np_doc_words = np.asarray(doc_words)\r\n",
        "    w, c = np.unique(np_doc_words, return_counts=True)\r\n",
        "    dictionary_test[doc_num] = {}\r\n",
        "    for i in range(len(w)):\r\n",
        "        dictionary_test[doc_num][w[i]] = c[i]\r\n",
        "    doc_num = doc_num + 1"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skzNA9kuuctq"
      },
      "source": [
        "#now we make a 2D array having the frequency of each word of our feature set in each individual documents\r\n",
        "\r\n",
        "X_test = []\r\n",
        "for k in dictionary_test.keys():\r\n",
        "    row = []\r\n",
        "    for f in features:\r\n",
        "        if(f in dictionary_test[k].keys()):\r\n",
        "            #if word f is present in the dictionary of the document as a key, its value is copied\r\n",
        "            #this gives us no. of occurences\r\n",
        "            row.append(dictionary_test[k][f]) \r\n",
        "        else:\r\n",
        "            #if not present, the no. of occurences is zero\r\n",
        "            row.append(0)\r\n",
        "    X_test.append(row)"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHUT4HISugXl"
      },
      "source": [
        "X_test = np.asarray(X_test)\r\n",
        "Y_test = np.asarray(Y_test)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ntWpxSlui9a",
        "outputId": "1eeaa318-f28e-4980-9073-97e414357fb8"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_k_paqzukrr",
        "outputId": "fc07685c-1749-488d-da60-ae3ce490230f"
      },
      "source": [
        "len(Y_test)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgxaLTC7unw7",
        "outputId": "4b2c665d-e768-451d-ff31-70392edab6e6"
      },
      "source": [
        "#fitting the model \r\n",
        "\r\n",
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "clf = MultinomialNB()\r\n",
        "clf.fit(X_train, Y_train)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj-8FPfvuoWz"
      },
      "source": [
        "Y_predict = clf.predict(X_test)"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY8iHjuaus_c",
        "outputId": "73128cb5-a9ae-4934-8292-df68e77686db"
      },
      "source": [
        "#accuracy for test data\r\n",
        "clf.score(X_test, Y_test)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7751455531017868"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uAFsednuxoy",
        "outputId": "d121bade-f138-494e-ff36-6bbc315390ae"
      },
      "source": [
        "#printing report on test\r\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\r\n",
        "print(classification_report(Y_test, Y_predict))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.74      0.76      0.75       251\n",
            "           comp.graphics       0.66      0.66      0.66       267\n",
            " comp.os.ms-windows.misc       0.65      0.64      0.65       256\n",
            "comp.sys.ibm.pc.hardware       0.60      0.65      0.63       233\n",
            "   comp.sys.mac.hardware       0.71      0.71      0.71       267\n",
            "          comp.windows.x       0.79      0.70      0.74       245\n",
            "            misc.forsale       0.71      0.68      0.69       249\n",
            "               rec.autos       0.73      0.82      0.77       236\n",
            "         rec.motorcycles       0.81      0.88      0.84       234\n",
            "      rec.sport.baseball       0.87      0.85      0.86       250\n",
            "        rec.sport.hockey       0.84      0.96      0.90       228\n",
            "               sci.crypt       0.88      0.92      0.90       239\n",
            "         sci.electronics       0.73      0.69      0.71       272\n",
            "                 sci.med       0.94      0.85      0.89       246\n",
            "               sci.space       0.91      0.85      0.88       259\n",
            "  soc.religion.christian       0.85      0.89      0.87       237\n",
            "      talk.politics.guns       0.77      0.88      0.82       244\n",
            "   talk.politics.mideast       0.95      0.91      0.93       260\n",
            "      talk.politics.misc       0.77      0.71      0.74       276\n",
            "      talk.religion.misc       0.59      0.53      0.56       232\n",
            "\n",
            "                accuracy                           0.78      4981\n",
            "               macro avg       0.78      0.78      0.78      4981\n",
            "            weighted avg       0.78      0.78      0.77      4981\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWHiHGRcu86j"
      },
      "source": [
        "Y_predict_tr = clf.predict(X_train)"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKz13UqFu-7a",
        "outputId": "2688cdda-ca70-4e38-df10-2de7d456944f"
      },
      "source": [
        "#accuracy for training data\r\n",
        "\r\n",
        "clf.score(X_train, Y_train)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8584621561935354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X9p00MMvB7D",
        "outputId": "e08dfc09-1407-42e2-997e-ae31f2167bc6"
      },
      "source": [
        "#printing report on training set\r\n",
        "print(classification_report(Y_train, Y_predict_tr))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.80      0.87      0.83       742\n",
            "           comp.graphics       0.80      0.78      0.79       728\n",
            " comp.os.ms-windows.misc       0.80      0.83      0.81       744\n",
            "comp.sys.ibm.pc.hardware       0.78      0.78      0.78       762\n",
            "   comp.sys.mac.hardware       0.79      0.83      0.81       718\n",
            "          comp.windows.x       0.89      0.83      0.86       751\n",
            "            misc.forsale       0.82      0.82      0.82       747\n",
            "               rec.autos       0.86      0.89      0.87       760\n",
            "         rec.motorcycles       0.88      0.93      0.91       766\n",
            "      rec.sport.baseball       0.91      0.93      0.92       743\n",
            "        rec.sport.hockey       0.91      0.96      0.94       766\n",
            "               sci.crypt       0.92      0.92      0.92       760\n",
            "         sci.electronics       0.85      0.84      0.85       722\n",
            "                 sci.med       0.95      0.87      0.90       751\n",
            "               sci.space       0.93      0.90      0.91       738\n",
            "  soc.religion.christian       0.93      0.92      0.92       760\n",
            "      talk.politics.guns       0.84      0.91      0.88       756\n",
            "   talk.politics.mideast       0.94      0.93      0.93       740\n",
            "      talk.politics.misc       0.81      0.77      0.79       724\n",
            "      talk.religion.misc       0.77      0.66      0.71       765\n",
            "\n",
            "                accuracy                           0.86     14943\n",
            "               macro avg       0.86      0.86      0.86     14943\n",
            "            weighted avg       0.86      0.86      0.86     14943\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}